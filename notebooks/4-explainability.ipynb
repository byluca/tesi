{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Explainability delle GNN nella segmentazione dei tumori cerebrali\n",
    "\n",
    "Questo notebook implementa e analizza vari metodi di explainability per modelli GNN applicati alla segmentazione dei tumori cerebrali.\n",
    "I metodi implementati includono GNNExplainer e GradCAM.\n",
    "\"\"\"\n",
    "\n",
    "# Importazione delle librerie necessarie\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from sys import platform\n",
    "\n",
    "# Impostazione dei percorsi\n",
    "_base_path = '\\\\'.join(os.getcwd().split('\\\\')[:-1]) + '\\\\' if platform == 'win32' else '/'.join(os.getcwd().split('/')[:-1]) + '/'\n",
    "sys.path.append(_base_path)\n",
    "\n",
    "# Importare le librerie necessarie\n",
    "from monai.utils import set_determinism\n",
    "from src.helpers.config import get_config\n",
    "from src.models.gnn import GraphSAGE, GAT, ChebNet\n",
    "from torch_geometric.explain import Explainer, ModelConfig, ThresholdConfig\n",
    "# Rimuoviamo PGExplainer e usiamo solo GNNExplainer\n",
    "from torch_geometric.explain.algorithm import GNNExplainer\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Definizione dei percorsi\n",
    "_config = get_config()\n",
    "data_path = os.path.join(_base_path, _config.get('DATA_FOLDER'))\n",
    "graph_path = os.path.join(data_path, _config.get('GRAPH_FOLDER'))\n",
    "saved_path = os.path.join(_base_path, _config.get('SAVED_FOLDER'))\n",
    "reports_path = os.path.join(_base_path, _config.get('REPORT_FOLDER'))\n",
    "logs_path = os.path.join(_base_path, _config.get('LOG_FOLDER'))\n",
    "\n",
    "if platform == 'win32':\n",
    "    data_path = data_path.replace('/', '\\\\')\n",
    "    graph_path = graph_path.replace('/', '\\\\')\n",
    "    saved_path = saved_path.replace('/', '\\\\')\n",
    "    reports_path = reports_path.replace('/', '\\\\')\n",
    "    logs_path = logs_path.replace('/', '\\\\')\n",
    "\n",
    "# Impostare un seed per la riproducibilità\n",
    "set_determinism(seed=3)\n",
    "random.seed(3)\n",
    "np.random.seed(3)\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Definizione dei parametri del modello\n",
    "# PARAMETRI CONDIVISI\n",
    "num_node_features = 50          # Dimensione feature di input\n",
    "num_classes = 4                 # Numero di classi di output\n",
    "lr = 1e-4                       # Learning rate per l'ottimizzatore\n",
    "weight_decay = 1e-5             # Weight decay per l'ottimizzatore\n",
    "dropout = .0                    # Probabilità di dropout (per features)\n",
    "hidden_channels = [512, 512, 512, 512, 512, 512, 512]  # Unità nascoste\n",
    "\n",
    "# PARAMETRI GRAPHSAGE\n",
    "aggr = 'mean'                   # Operazione di aggregazione\n",
    "\n",
    "# PARAMETRI GAT\n",
    "heads = 14                      # Numero di attention heads\n",
    "attention_dropout = .1          # Probabilità di dropout (per attention)\n",
    "\n",
    "# PARAMETRI CHEBNET\n",
    "k = 4                           # Ordine polinomiale Chebyshev\n",
    "\n",
    "# Creazione del modello da utilizzare\n",
    "model = ChebNet(\n",
    "    in_channels=num_node_features,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=num_classes,\n",
    "    dropout=dropout,\n",
    "    K=k\n",
    ")\n",
    "print(f\"Modello creato: {model.__class__.__name__}\")\n",
    "\n",
    "# Funzione per trovare e caricare un grafo per l'analisi\n",
    "def find_and_load_graph(subject_id=None):\n",
    "    \"\"\"\n",
    "    Trova e carica un grafo per l'analisi.\n",
    "    \n",
    "    Args:\n",
    "        subject_id: ID specifico del soggetto da caricare, se None ne verrà scelto uno casualmente\n",
    "        \n",
    "    Returns:\n",
    "        data: Il grafo caricato\n",
    "        subject_id: L'ID del soggetto caricato\n",
    "    \"\"\"\n",
    "    if subject_id is None:\n",
    "        # Trova le cartelle dei soggetti che contengono grafi\n",
    "        subject_dirs = [d for d in os.listdir(graph_path) if os.path.isdir(os.path.join(graph_path, d))]\n",
    "        valid_subjects = []\n",
    "        \n",
    "        # Cerca i primi 10 soggetti che hanno file .graph\n",
    "        for subject in subject_dirs[:100]:  # Limita la ricerca per efficienza\n",
    "            graph_file = os.path.join(graph_path, subject, f\"{subject}.graph\")\n",
    "            if os.path.isfile(graph_file):\n",
    "                valid_subjects.append(subject)\n",
    "                if len(valid_subjects) >= 10:\n",
    "                    break\n",
    "        \n",
    "        if not valid_subjects:\n",
    "            raise FileNotFoundError(\"Nessun grafo trovato nella directory data/graphs/\")\n",
    "        \n",
    "        # Scegli un soggetto casuale\n",
    "        subject_id = random.choice(valid_subjects)\n",
    "    \n",
    "    # Carica il grafo\n",
    "    graph_file = os.path.join(graph_path, subject_id, f\"{subject_id}.graph\")\n",
    "    if not os.path.isfile(graph_file):\n",
    "        raise FileNotFoundError(f\"File grafo non trovato per il soggetto {subject_id}\")\n",
    "    \n",
    "    print(f\"Caricamento grafo: {graph_file}\")\n",
    "    data = torch.load(graph_file)\n",
    "    \n",
    "    return data, subject_id\n",
    "\n",
    "# Carica un grafo specifico con alta accuratezza (come visto nel precedente test)\n",
    "subject_id = \"BraTS-GLI-01166-000\"  # Grafo con accuratezza 100%\n",
    "try:\n",
    "    data, subject_id = find_and_load_graph(subject_id)\n",
    "    print(f\"Grafo caricato con successo: {subject_id}\")\n",
    "    print(f\"Numero di nodi: {data.x.shape[0]}\")\n",
    "    print(f\"Numero di archi: {data.edge_index.shape[1]}\")\n",
    "    print(f\"Numero di features per nodo: {data.x.shape[1]}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Errore: {e}\")\n",
    "    print(\"Tentativo di caricamento di un grafo alternativo...\")\n",
    "    data, subject_id = find_and_load_graph(None)\n",
    "    print(f\"Grafo alternativo caricato: {subject_id}\")\n",
    "\n",
    "# Carica il modello pre-addestrato\n",
    "model_files = [f for f in os.listdir(saved_path) if 'CHEBNET' in f and f.endswith('_best.pth')]\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(\"Nessun modello ChebNet pre-addestrato trovato nella directory saved/\")\n",
    "\n",
    "latest_model = model_files[-1]\n",
    "print(f\"Utilizzo del modello pre-addestrato: {latest_model}\")\n",
    "model.load_state_dict(torch.load(os.path.join(saved_path, latest_model), map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Verifica dell'accuratezza sul grafo caricato\n",
    "with torch.no_grad():\n",
    "    outputs = model(data.x, data.edge_index.type(torch.int64))\n",
    "    predicted_labels = outputs.argmax(dim=1)\n",
    "    \n",
    "    # Visualizza le informazioni per debug\n",
    "    print(\"\\n--- DEBUG INFORMAZIONI DATI ---\")\n",
    "    print(f\"Tipo di data.y: {type(data.y)}\")\n",
    "    if hasattr(data.y, 'shape'):\n",
    "        print(f\"Forma di data.y: {data.y.shape}\")\n",
    "    if hasattr(data.y, 'dtype'):\n",
    "        print(f\"Tipo di dati di data.y: {data.y.dtype}\")\n",
    "    print(f\"Tipo di predicted_labels: {type(predicted_labels)}\")\n",
    "    print(f\"Forma di predicted_labels: {predicted_labels.shape}\")\n",
    "    print(f\"Tipo di dati di predicted_labels: {predicted_labels.dtype}\")\n",
    "    \n",
    "    try:\n",
    "        # Prova a estrarre il primo elemento di data.y per vedere se funziona\n",
    "        if len(data.y) > 0:\n",
    "            first_y = data.y[0]\n",
    "            print(f\"Primo elemento di data.y: {first_y}\")\n",
    "            if hasattr(first_y, 'shape'):\n",
    "                print(f\"Forma del primo elemento di data.y: {first_y.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nell'accesso a data.y: {e}\")\n",
    "    \n",
    "    print(\"--------------------------------\\n\")\n",
    "    \n",
    "    # Crea un tensore delle classi da usare, contenente le predizioni del modello\n",
    "    node_classes = predicted_labels.clone()\n",
    "    \n",
    "    # Calcola l'accuratezza usando predicted_labels e data.y se possibile\n",
    "    try:\n",
    "        accuracy = (predicted_labels == data.y).float().mean().item()\n",
    "        print(f\"Accuratezza sul grafo {subject_id}: {accuracy:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Impossibile calcolare l'accuratezza usando data.y: {e}\")\n",
    "        print(\"Usando solo le predizioni per l'analisi\")\n",
    "    \n",
    "    class_counts = torch.bincount(predicted_labels, minlength=num_classes)\n",
    "    print(f\"Distribuzione classi predette: {class_counts.numpy()}\")\n",
    "\n",
    "# Configurazione GNNExplainer per la spiegabilità\n",
    "print(\"\\nConfigurazione GNNExplainer e generazione della spiegazione...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    mode=\"multiclass_classification\",\n",
    "    task_level=\"node\",\n",
    "    return_type=\"log_probs\",\n",
    ")\n",
    "\n",
    "gnn_explainer = GNNExplainer(epochs=200, lr=1e-4)\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=gnn_explainer,\n",
    "    explanation_type=\"model\",\n",
    "    model_config=model_config,\n",
    "    node_mask_type=\"attributes\",\n",
    "    edge_mask_type=None,\n",
    "    threshold_config=ThresholdConfig(threshold_type=\"topk\", value=int(0.5 * data.x.shape[0]))\n",
    ")\n",
    "\n",
    "# Genera la spiegazione\n",
    "explanation = explainer(\n",
    "    x=data.x,\n",
    "    edge_index=data.edge_index.type(torch.int64)\n",
    ")\n",
    "\n",
    "print(f\"Spiegazione generata in {time.time() - start_time:.2f} secondi\")\n",
    "\n",
    "# DEBUG: Stampa informazioni sul node_mask\n",
    "print(f\"Tipo di explanation.node_mask: {type(explanation.node_mask)}\")\n",
    "print(f\"Forma di explanation.node_mask: {explanation.node_mask.shape}\")\n",
    "if explanation.node_mask.numel() > 0:\n",
    "    print(f\"Primo elemento di explanation.node_mask: {explanation.node_mask[0]}\")\n",
    "    print(f\"Forma del primo elemento: {explanation.node_mask[0].shape if hasattr(explanation.node_mask[0], 'shape') else 'scalar'}\")\n",
    "\n",
    "# Funzione per estrarre valori scalari in sicurezza\n",
    "def safe_scalar_value(tensor):\n",
    "    \"\"\"Estrae in modo sicuro un valore scalare da un tensore.\"\"\"\n",
    "    if isinstance(tensor, (int, float)):\n",
    "        return float(tensor)\n",
    "    if hasattr(tensor, 'item'):\n",
    "        if tensor.numel() == 1:\n",
    "            return tensor.item()\n",
    "        else:\n",
    "            return float(tensor.mean().item())\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        if tensor.size == 1:\n",
    "            return float(tensor.item())\n",
    "        else:\n",
    "            return float(tensor.mean())\n",
    "    return float(tensor)  # Tenta una conversione diretta\n",
    "\n",
    "# Visualizzazione dei risultati delle spiegazioni\n",
    "print(\"\\nVisualizzazione dei risultati delle spiegazioni...\")\n",
    "\n",
    "# Estrai i valori di importanza dei nodi in modo sicuro\n",
    "node_importances = []\n",
    "for i in range(len(explanation.node_mask)):\n",
    "    node_importances.append(safe_scalar_value(explanation.node_mask[i]))\n",
    "node_importances = np.array(node_importances)\n",
    "\n",
    "# 1. Visualizza la distribuzione dei punteggi di importanza dei nodi\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(node_importances, bins=50)\n",
    "plt.title('Distribuzione delle importanze dei nodi')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Conteggio')\n",
    "\n",
    "# 2. Visualizza l'importanza media per classe\n",
    "classes = [\"Non-tumore\", \"NCR\", \"ED\", \"ET\"]\n",
    "class_importances = []\n",
    "for i in range(len(classes)):\n",
    "    # Converti il tensore PyTorch in array NumPy\n",
    "    node_classes_np = node_classes.cpu().numpy()\n",
    "    mask = (node_classes_np == i)\n",
    "    if np.any(mask):\n",
    "        # Estrai le importanze per questa classe\n",
    "        class_node_importances = [node_importances[j] for j in range(len(mask)) if mask[j]]\n",
    "        class_importances.append(np.mean(class_node_importances) if class_node_importances else 0)\n",
    "    else:\n",
    "        class_importances.append(0)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(classes, class_importances)\n",
    "plt.title('Importanza media per classe')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Importanza media')\n",
    "\n",
    "# 3. Visualizza i top nodi più importanti\n",
    "top_n = 10  # Numero di nodi top da visualizzare\n",
    "\n",
    "# Trova gli indici dei top_n nodi - corretti per evitare stride negativi\n",
    "sorted_indices = np.flip(np.argsort(node_importances))  # Usa flip invece di [::-1]\n",
    "top_n_indices = sorted_indices[:top_n]\n",
    "top_n_importances = [node_importances[i] for i in top_n_indices]\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(range(top_n), top_n_importances)\n",
    "plt.title(f'Top {top_n} nodi per importanza')\n",
    "plt.xlabel('Indice del nodo (ordinato)')\n",
    "plt.ylabel('Importanza')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisi dettagliata dei top nodi\n",
    "print(\"\\nAnalisi dettagliata dei nodi più importanti:\")\n",
    "print(f\"{'Indice':10}{'Importanza':15}{'Classe predetta':15}\")\n",
    "print(\"-\" * 40)\n",
    "for i, idx in enumerate(top_n_indices):\n",
    "    node_importance = node_importances[idx]\n",
    "    try:\n",
    "        # Converti l'indice in intero e accedi al tensore\n",
    "        class_idx = node_classes[int(idx)].item()\n",
    "        pred_class = classes[class_idx]\n",
    "        print(f\"{int(idx):10}{node_importance:.6f}{'':8}{pred_class:15}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{int(idx):10}{node_importance:.6f}{'':8}ERRORE: {e}\")\n",
    "\n",
    "# Analisi delle feature più importanti per i top nodi\n",
    "if hasattr(explanation, 'feature_importance') and explanation.feature_importance is not None:\n",
    "    # Se l'explainer ha calcolato l'importanza delle feature\n",
    "    # Estrai i valori di importanza delle feature in modo sicuro\n",
    "    feature_importances = []\n",
    "    for i in range(explanation.feature_importance.shape[1]):\n",
    "        feature_importances.append(safe_scalar_value(explanation.feature_importance[:, i]))\n",
    "    feature_importances = np.array(feature_importances)\n",
    "    \n",
    "    top_features = np.flip(np.argsort(feature_importances))[:5]\n",
    "    \n",
    "    print(\"\\nAnalisi delle feature più importanti:\")\n",
    "    print(f\"{'Indice feature':15}{'Importanza':15}\")\n",
    "    print(\"-\" * 30)\n",
    "    for idx in top_features:\n",
    "        print(f\"{idx:15}{feature_importances[idx]:.6f}\")\n",
    "else:\n",
    "    # Alternativa: analizza le feature nei nodi più importanti\n",
    "    print(\"\\nAnalisi delle feature nei nodi più importanti:\")\n",
    "    # Converti l'array NumPy in lista o tensore PyTorch per l'indicizzazione\n",
    "    top_n_indices_list = [int(i) for i in top_n_indices]\n",
    "    top_node_features = data.x[top_n_indices_list]\n",
    "    mean_features = torch.mean(top_node_features, dim=0).cpu().numpy()\n",
    "    top_mean_features = np.flip(np.argsort(mean_features))[:5]\n",
    "    \n",
    "    print(f\"{'Indice feature':15}{'Valore medio':15}\")\n",
    "    print(\"-\" * 30)\n",
    "    for idx in top_mean_features:\n",
    "        print(f\"{idx:15}{mean_features[idx]:.6f}\")\n",
    "\n",
    "# Confronto tra classi per importanza - usando importanze già estratte\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Converti node_classes in numpy una volta sola\n",
    "node_classes_np = node_classes.cpu().numpy()\n",
    "\n",
    "# Raccogliamo le importanze per ogni classe\n",
    "class_importance_distributions = []\n",
    "class_labels = []\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_mask = (node_classes_np == i)\n",
    "    if np.any(class_mask):\n",
    "        class_importances = [node_importances[j] for j in range(len(class_mask)) if class_mask[j]]\n",
    "        class_importance_distributions.append(class_importances)\n",
    "        class_labels.append(class_name)\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(class_importances, bins=30, alpha=0.5, label=class_name)\n",
    "\n",
    "# Box plot per confronto più chiaro\n",
    "plt.subplot(1, 2, 2)\n",
    "if class_importance_distributions:  # Assicurati che ci sia almeno una distribuzione\n",
    "    plt.boxplot(class_importance_distributions, labels=class_labels)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Distribuzione importanza per classe')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Conteggio')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Box plot importanza per classe')\n",
    "plt.ylabel('Importanza')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Saltiamo PGExplainer poiché non è compatibile\n",
    "print(\"\\nPGExplainer non è compatibile con questa configurazione, lo saltiamo.\")\n",
    "\n",
    "# Creiamo dati fittizi per PGExplainer per mantenere la compatibilità con il resto del codice\n",
    "pg_node_importances = np.zeros_like(node_importances)\n",
    "top_pg = set(range(min(50, len(node_importances))))\n",
    "\n",
    "# Implementazione di GradCAM \n",
    "print(\"\\nImplementazione di GradCAM (versione custom)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Implementazione semplificata di GradCAM\n",
    "x_grad = data.x.clone().detach().requires_grad_(True)\n",
    "with torch.enable_grad():\n",
    "    outputs = model(x_grad, data.edge_index.type(torch.int64))\n",
    "    model.zero_grad()\n",
    "    target_class = outputs.argmax(dim=1)\n",
    "    # Usa gather per selezionare la classe predetta per ogni nodo\n",
    "    loss = outputs.gather(1, target_class.unsqueeze(1)).sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calcola l'importanza per ogni nodo come media del gradiente\n",
    "    gradcam_importances = []\n",
    "    for i in range(len(x_grad)):\n",
    "        grad_value = x_grad.grad[i].abs().mean().item()\n",
    "        gradcam_importances.append(grad_value)\n",
    "    gradcam_importances = np.array(gradcam_importances)\n",
    "\n",
    "print(f\"GradCAM completato in {time.time() - start_time:.2f} secondi\")\n",
    "\n",
    "# Visualizza GradCAM\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(gradcam_importances, bins=50)\n",
    "plt.title('Distribuzione importanze (GradCAM)')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Conteggio')\n",
    "\n",
    "# Importanza media per classe con GradCAM\n",
    "gradcam_class_importances = []\n",
    "for i in range(len(classes)):\n",
    "    mask = (node_classes_np == i)  # Usa l'array numpy già convertito prima\n",
    "    if np.any(mask):\n",
    "        class_importances = [gradcam_importances[j] for j in range(len(mask)) if mask[j]]\n",
    "        gradcam_class_importances.append(np.mean(class_importances) if class_importances else 0)\n",
    "    else:\n",
    "        gradcam_class_importances.append(0)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(classes, gradcam_class_importances)\n",
    "plt.title('Importanza media per classe (GradCAM)')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Importanza media')\n",
    "\n",
    "# Top nodi secondo GradCAM\n",
    "top_gradcam_indices = np.flip(np.argsort(gradcam_importances))[:top_n]\n",
    "top_gradcam_importances = [gradcam_importances[i] for i in top_gradcam_indices]\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(range(top_n), top_gradcam_importances)\n",
    "plt.title(f'Top {top_n} nodi (GradCAM)')\n",
    "plt.xlabel('Indice del nodo (ordinato)')\n",
    "plt.ylabel('Importanza')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confronto tra i metodi disponibili (GNNExplainer e GradCAM)\n",
    "print(\"\\nConfronto tra GNNExplainer e GradCAM:\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Confronto distribuzioni\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(node_importances, bins=30, alpha=0.5, label='GNNExplainer')\n",
    "plt.hist(gradcam_importances, bins=30, alpha=0.5, label='GradCAM')\n",
    "plt.title('Confronto distribuzioni di importanza')\n",
    "plt.xlabel('Importanza')\n",
    "plt.ylabel('Conteggio')\n",
    "plt.legend()\n",
    "\n",
    "# Boxplot comparativo\n",
    "plt.subplot(1, 2, 2)\n",
    "data_to_plot = [\n",
    "    node_importances,\n",
    "    gradcam_importances\n",
    "]\n",
    "plt.boxplot(data_to_plot, labels=['GNNExplainer', 'GradCAM'])\n",
    "plt.title('Confronto statistico dei metodi')\n",
    "plt.ylabel('Importanza')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sovrapposizione dei top nodi rilevati dai diversi metodi\n",
    "print(\"\\nSovrapposizione tra i top nodi identificati dai diversi metodi:\")\n",
    "top_k = 50  # Considera i top 50 nodi\n",
    "\n",
    "# Ottieni i top k nodi per ciascun metodo usando gli array già elaborati - evita stride negativi\n",
    "top_gnn = set(np.flip(np.argsort(node_importances))[:top_k])\n",
    "top_gradcam = set(np.flip(np.argsort(gradcam_importances))[:top_k])\n",
    "\n",
    "# Calcola le sovrapposizioni\n",
    "overlap_gnn_gradcam = len(top_gnn.intersection(top_gradcam))\n",
    "\n",
    "print(f\"Sovrapposizione GNNExplainer-GradCAM: {overlap_gnn_gradcam} nodi ({overlap_gnn_gradcam/top_k*100:.1f}%)\")\n",
    "\n",
    "# Analisi per classe dei nodi identificati da ciascun metodo\n",
    "print(\"\\nAnalisi per classe dei nodi top identificati da ciascun metodo:\")\n",
    "methods = {\n",
    "    \"GNNExplainer\": top_gnn,\n",
    "    \"GradCAM\": top_gradcam\n",
    "}\n",
    "\n",
    "for method_name, top_nodes in methods.items():\n",
    "    class_counts = [0] * len(classes)\n",
    "    for node_idx in top_nodes:\n",
    "        try:\n",
    "            # Usa l'array NumPy già convertito\n",
    "            class_idx = node_classes_np[int(node_idx)]\n",
    "            class_counts[int(class_idx)] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il conteggio delle classi per il nodo {node_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{method_name}:\")\n",
    "    for i, class_name in enumerate(classes):\n",
    "        print(f\"  {class_name}: {class_counts[i]} nodi ({class_counts[i]/top_k*100:.1f}%)\")\n",
    "\n",
    "# Conclusioni\n",
    "print(\"\\nConclusioni:\")\n",
    "print(\"Questo notebook ha dimostrato l'applicazione di diverse tecniche di explainability per GNN\")\n",
    "print(\"nella segmentazione dei tumori cerebrali. I risultati mostrano:\")\n",
    "print(\"1. Differenze significative tra i diversi metodi di explainability\")\n",
    "print(\"2. Diversi livelli di importanza attribuiti alle varie classi di tessuto\")\n",
    "print(\"3. Identificazione di nodi chiave che influenzano maggiormente le predizioni del modello\")\n",
    "print(\"\\nQuesti risultati sottolineano l'importanza dell'utilizzo di tecniche di explainability\")\n",
    "print(\"per comprendere le decisioni dei modelli GNN nella segmentazione dei tumori cerebrali.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
